akka {
  loglevel = INFO
}

akka.http {
  client {
    user-agent-header = ${?APPLICATION_NAME}
  }

  host-connection-pool {
    max-retries: 10
    max-connections: 100
    max-open-requests: 256
  }
}

# Properties for akka.kafka.ConsumerSettings can be
# defined in this section or a configuration section with
# the same layout.
akka.kafka.consumer {
  # Tuning property of scheduled polls.
  poll-interval = 500ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that the thread that
  # is executing the stage will be blocked.
  poll-timeout = ${?KAFKA_CONSUMER_POLL_TIMEOUT}

  # The stage will await outstanding offset commit requests before
  # shutting down, but if that takes longer than this timeout it will
  # stop forcefully.
  stop-timeout = 30s

  # How long to wait for `KafkaConsumer.close`
  close-timeout = 300s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `CommitTimeoutException`.
  commit-timeout = 15s

  # If commits take longer than this time a warning is logged
  commit-time-warning = 3s

  # If for any reason `KafkaConsumer.poll` blocks for longer than the configured
  # poll-timeout then it is forcefully woken up with `KafkaConsumer.wakeup`.
  # The KafkaConsumerActor will throw
  # `org.apache.kafka.common.errors.WakeupException` which will be ignored
  # until `max-wakeups` limit gets exceeded.
  wakeup-timeout = 3s

  # After exceeding maxinum wakeups the consumer will stop and the stage and fail.
  # Setting it to 0 will let it ignore the wakeups and try to get the polling done forever.
  max-wakeups = 50

  # If set to a finite duration, the consumer will re-send the last committed offsets periodically
  # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
  commit-refresh-interval = infinite

  # If enabled, log stack traces before waking up the KafkaConsumer to give
  # some indication why the KafkaConsumer is not honouring the `poll-timeout`
  wakeup-debug = false

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Disable auto-commit by default
    enable.auto.commit = false
    max.poll.records = ${?KAFKA_CONSUMER_MAX_POLL_RECORDS}
  }

  # Time to wait for pending requests when a partition is closed
  wait-close-partition = 500ms
}

kafka {
  host = ${?KAFKA_HOST}
  group-id = ${?KAFKA_GROUP_ID}
  topic = ${?KAFKA_TOPIC}
  avro-topic = ${?KAFKA_AVRO_TOPIC}
  poll-timeout = ${?KAFKA_CONSUMER_POLL_TIMEOUT}
  max-poll-records = ${?KAFKA_CONSUMER_MAX_POLL_RECORDS}
  filter-time-in-seconds = ${?KAFKA_FILTER_TIME_IN_SECONDS}
  schema-registry = ${?KAFKA_SCHEMA_REGISTRY}
}

ping-port = 8000

influx {
  log-level = "BASIC"
  log-level = ${?INFLUXDB_LOG_LEVEL}
  databases = [
    {
      host = ${?CLOUD_INFLUXDB_HOST}
      port = ${?CLOUD_INFLUXDB_PORT}
      database = ${?CLOUD_INFLUXDB_DATABASE}
      username = ${?CLOUD_INFLUXDB_USERNAME}
      password = ${?CLOUD_INFLUXDB_PASSWORD}
      retention-policy = "twelve_weeks"
      measurement = "telemetry_raw"
    }
  ],
  bad-data-databases = [
    {
      host = ${?CLOUD_INFLUXDB_HOST}
      port = ${?CLOUD_INFLUXDB_PORT}
      database = ${?CLOUD_INFLUXDB_DATABASE}
      username = ${?CLOUD_INFLUXDB_USERNAME}
      password = ${?CLOUD_INFLUXDB_PASSWORD}
      retention-policy = "twenty_four_weeks"
      measurement = "telemetry_filtered"
    }
  ]
}

telemetry {
  filters {
    timestamp-threshold-in-days = 2
  }
}

api {
  ping-port = 8000
}

# Kamon configuration
kamon {
  system-metrics {
    sigar-enabled = false
    jmx-enabled = false
  }

  metric {
    filters {
      akka-actor {
        includes = [  ]
        excludes = [ "*/system/**", "*/user/IO-**", "*kamon*", "*/kamon/*", "**/kamon/**", "kamon/**" ]
      }

      akka-router {
        includes = [  ]
        excludes = [ "*/system/**", "*/user/IO-**", "*kamon*", "*/kamon/*", "**/kamon/**", "kamon/**" ]
      }

      akka-dispatcher {
        includes = [  ]
        excludes = [ "*/system/**", "*/user/IO-**", "*kamon*", "*/kamon/*", "**/kamon/**", "kamon/**" ]
      }

      trace {
        includes = [ "**" ]
        excludes = [ ]
      }

      http {
        includes = [ "**" ]
        excludes = [ ]
      }

      kafka-consumer {
        includes = [ "**" ]
        excludes = [ ]
      }
    }
  }

  statsd {

    report-system-metrics = false

    # Hostname and port in which your StatsD is running. Remember that StatsD packets are sent using UDP and
    # setting unreachable hosts and/or not open ports wont be warned by the Kamon, your data wont go anywhere.
    hostname = ${?STATSD_HOST}
    port = ${?STATSD_PORT}

    # Interval between metrics data flushes to StatsD. It's value must be equal or greater than the
    # kamon.metric.tick-interval setting.
    # flush-interval = 10 seconds

    # Subscription patterns used to select which metrics will be pushed to StatsD. Note that first, metrics
    # collection for your desired entities must be activated under the kamon.metrics.filters settings.
    subscriptions {
      histogram       = [ "**" ]
      min-max-counter = [ "**" ]
      gauge           = [ "**" ]
      counter         = [ "**" ]
      trace           = [ "**" ]
      trace-segment   = [ "**" ]
      akka-actor      = [ "**" ]
      akka-dispatcher = [ "**" ]
      akka-router     = [ "**" ]
      system-metric   = [ "**" ]
      http-server     = [ "**" ]
      http = [ "**" ]
      kafka-consumer = [ "**" ]
    }

    # FQCN of the implementation of `kamon.statsd.MetricKeyGenerator` to be instantiated and used for assigning
    # metric names. The implementation must have a single parameter constructor accepting a `com.typesafe.config.Config`.
    metric-key-generator = kamon.statsd.SimpleMetricKeyGenerator

    simple-metric-key-generator {

      # The default namespacing scheme for metrics follows
      # this pattern:
      #    application.host.entity.entity-name.metric-name
      application = "kamon"

      include-hostname = false

      # When the sections that make up the metric names have special characters like dots (very common in dispatcher
      # names) or forward slashes (all actor metrics) we need to sanitize those values before sending them to StatsD
      # with one of the following strategies:
      #   - normalize: changes ': ' to '-' and ' ', '/' and '.' to '_'.
      #   - percent-encode: percent encode the section on the metric name. Please note that StatsD doesn't support
      #     percent encoded metric names, this option is only useful if using our docker image which has a patched
      #     version of StatsD or if you are running your own, customized version of StatsD that supports this.
      metric-name-normalization-strategy = normalize
    }
  }
}
