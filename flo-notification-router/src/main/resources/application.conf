akka {
  loglevel = "INFO"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
}

akka.http {
  client {
    user-agent-header = ${?APPLICATION_NAME}
  }

  host-connection-pool {
    max-retries: 10
    max-connections: 100
    max-open-requests: 256
  }
}

redis {
  host = ${?REDIS_HOST}
  port = ${?REDIS_PORT}
}

push {
  enabled = true
  enabled = ${?PUSH_ENABLED}
}

cs {
  email = 1
}

kafka {
  host = "foo"
  group-id = "foo-consumer"
  topic = "foo-v2"
  email-producer {
    topic = "foo-v2"
    topic-v2 = "foo-v3"
  }
  sms-producer {
    topic = "foo-v2"
  }
  alarm-notification-status {
    topic = "foo-v2"
  }
  external-actions {
    valve-status {
      topic = "foo-v2"
    }
  }
  scheduled-notifications {
    scheduled-task {
      topic = "foo-v2"
    }

  }
  voice-producer {
    topic = "foo-woice"
  }
  filter-time-in-seconds = ${?KAFKA_FILTER_TIME_IN_SECONDS}
  poll-timeout = ${?KAFKA_CONSUMER_POLL_TIMEOUT}
  max-poll-records = ${?KAFKA_CONSUMER_MAX_POLL_RECORDS}
  encryption = ${?KAFKA_ENCRYPTION_ENABLED}
}

flo {
  api {
    ping-port = 8000
    url = "foo"
    user = "foo"
    token = "foo"
    client {
      id = "i-am-a-foo"
      secret = "trust-no-one"
    }
  }

  //Prduction  =  APNS
  sns {
    apple {
      default-app-arn = "foo"
      push-notification-platform = "APNS"
    }
    android {
      default-app-arn = "foo"
    }
  }
  //FLO_GRAVEYARD_TIME
  graveyard-time {
    //FLO_GRAVEYARD_TIME_STARTS_HOUR_OF_THE_DAY
    starts-hour-of-the-day: 0
    //FLO_GRAVEYARD_TIME_ENDS_HOUR_OF_THE_DAY
    ends-hour-of-the-day: 7
    //FLO_GRAVEYARD_TIME_ENABLED
    enabled: 1
    //FLO_GRAVEYARD_TIME_SEND_EMAILS
    send-emails: 1
    //FLO_GRAVEYARD_TIME_SEND_SMS
    send-sms: 0
    //FLO_GRAVEYARD_TIME_SEND_APP_NOTIFICATIONS
    send-app-notifications: 0
  }
}

flo-actors {
  number-of-workers {
    apple-push-notifications = 1
    android-push-notifications = 1
    decision-engine = 1
    kafka-reader = 1
    kafka-producer = 1
    kafka-reader-external-actions = 1
    external-actions = 1

  }

}

cipher {
  key-provider {
    bucket-region = ${?KEY_PROVIDER_BUCKET_REGION}
    bucket-name = ${?KEY_PROVIDER_BUCKET_NAME}
    key-path-template = ${?KEY_PROVIDER_KEY_PATH_TEMPLATE}
    key-id = ${?KEY_ID}
  }
}

# Kamon configuration
kamon {
  system-metrics {
    sigar-enabled = false
    jmx-enabled = false
  }

  metric {
    filters {
      akka-actor {
        includes = []
        excludes = ["*/system/**", "*/user/IO-**", "*kamon*", "*/kamon/*", "**/kamon/**", "kamon/**"]
      }

      akka-router {
        includes = []
        excludes = ["*/system/**", "*/user/IO-**", "*kamon*", "*/kamon/*", "**/kamon/**", "kamon/**"]
      }

      akka-dispatcher {
        includes = []
        excludes = ["*/system/**", "*/user/IO-**", "*kamon*", "*/kamon/*", "**/kamon/**", "kamon/**"]
      }

      trace {
        includes = ["**"]
        excludes = []
      }

      http {
        includes = ["**"]
        excludes = []
      }

      kafka-consumer {
        includes = [ "**" ]
        excludes = [ ]
      }
    }
  }

  statsd {

    report-system-metrics = false

    # Hostname and port in which your StatsD is running. Remember that StatsD packets are sent using UDP and
    # setting unreachable hosts and/or not open ports wont be warned by the Kamon, your data wont go anywhere.
    hostname = ${?STATSD_HOST}
    port = ${?STATSD_PORT}

    # Interval between metrics data flushes to StatsD. It's value must be equal or greater than the
    # kamon.metric.tick-interval setting.
    # flush-interval = 10 seconds

    # Subscription patterns used to select which metrics will be pushed to StatsD. Note that first, metrics
    # collection for your desired entities must be activated under the kamon.metrics.filters settings.
    subscriptions {
      histogram = ["**"]
      min-max-counter = ["**"]
      gauge = ["**"]
      counter = ["**"]
      trace = ["**"]
      trace-segment = ["**"]
      akka-actor = ["**"]
      akka-dispatcher = ["**"]
      akka-router = ["**"]
      system-metric = ["**"]
      http-server = ["**"]
      http = ["**"]
      kafka-consumer = ["**"]
    }

    # FQCN of the implementation of `kamon.statsd.MetricKeyGenerator` to be instantiated and used for assigning
    # metric names. The implementation must have a single parameter constructor accepting a `com.typesafe.config.Config`.
    metric-key-generator = kamon.statsd.SimpleMetricKeyGenerator

    simple-metric-key-generator {

      # The default namespacing scheme for metrics follows
      # this pattern:
      #    application.host.entity.entity-name.metric-name
      application = "kamon"

      include-hostname = false

      # When the sections that make up the metric names have special characters like dots (very common in dispatcher
      # names) or forward slashes (all actor metrics) we need to sanitize those values before sending them to StatsD
      # with one of the following strategies:
      #   - normalize: changes ': ' to '-' and ' ', '/' and '.' to '_'.
      #   - percent-encode: percent encode the section on the metric name. Please note that StatsD doesn't support
      #     percent encoded metric names, this option is only useful if using our docker image which has a patched
      #     version of StatsD or if you are running your own, customized version of StatsD that supports this.
      metric-name-normalization-strategy = normalize
    }
  }
}